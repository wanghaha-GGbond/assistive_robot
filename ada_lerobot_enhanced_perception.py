#!/usr/bin/env python3
"""
Ada LeRobot Enhanced Perception Model v2.0
å¢å¼ºç‰ˆAdaæœºå™¨äººæ„ŸçŸ¥æ¨¡å‹ - 10Må‚æ•°ç‰ˆæœ¬
ä¸“ä¸ºé¤é¥®åŠ©æ‰‹æœºå™¨äººè®¾è®¡çš„é«˜ç²¾åº¦å¤šæ¨¡æ€æ„ŸçŸ¥ç³»ç»Ÿ
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Dict, List, Optional, Tuple
import math

class TemporalAggregator(nn.Module):
    """æ—¶åºèšåˆæ¨¡å—ï¼Œç”¨äºå¤„ç†åºåˆ—æ•°æ®"""
    def __init__(self, method='mean'):
        super().__init__()
        self.method = method
    
    def forward(self, x):
        # è¾“å…¥å½¢çŠ¶: (batch, seq_len, features)
        # è¾“å‡ºå½¢çŠ¶: (batch, features)
        if self.method == 'mean':
            return x.mean(dim=1)
        elif self.method == 'max':
            return x.max(dim=1)[0]  # maxè¿”å›å€¼å’Œç´¢å¼•ï¼Œæˆ‘ä»¬åªè¦å€¼
        elif self.method == 'last':
            return x[:, -1]  # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥
        else:
            return x.mean(dim=1)

class EnhancedMultiHeadAttention(nn.Module):
    """å¢å¼ºç‰ˆå¤šå¤´æ³¨æ„åŠ›æœºåˆ¶"""
    def __init__(self, d_model=256, num_heads=16, dropout=0.1):
        super().__init__()
        self.d_model = d_model
        self.num_heads = num_heads
        self.head_dim = d_model // num_heads
        
        # å¢åŠ å¤´æ•°ä»¥è·å¾—æ›´ç»†ç²’åº¦çš„æ³¨æ„åŠ›
        self.query = nn.Linear(d_model, d_model)
        self.key = nn.Linear(d_model, d_model)
        self.value = nn.Linear(d_model, d_model)
        
        # ç›¸å¯¹ä½ç½®ç¼–ç  (è°ƒæ•´å¤§å°ä»¥é€‚åº”æ›´å°çš„åºåˆ—)
        max_relative_position = 32  # å‡å°‘ç›¸å¯¹ä½ç½®èŒƒå›´
        self.relative_position_encoding = nn.Parameter(
            torch.randn(2 * max_relative_position - 1, self.head_dim)
        )
        
        self.dropout = nn.Dropout(dropout)
        self.output_projection = nn.Linear(d_model, d_model)
        
    def forward(self, x, mask=None):
        batch_size, seq_len, d_model = x.shape
        
        # å¤šå¤´æŠ•å½±
        q = self.query(x).view(batch_size, seq_len, self.num_heads, self.head_dim)
        k = self.key(x).view(batch_size, seq_len, self.num_heads, self.head_dim)
        v = self.value(x).view(batch_size, seq_len, self.num_heads, self.head_dim)
        
        q = q.transpose(1, 2)  # [batch, heads, seq_len, head_dim]
        k = k.transpose(1, 2)
        v = v.transpose(1, 2)
        
        # ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›
        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)
        
        # æ·»åŠ ç›¸å¯¹ä½ç½®ç¼–ç  (åªæœ‰åœ¨åºåˆ—é•¿åº¦åˆé€‚æ—¶)
        if seq_len <= 64:  # é™åˆ¶ç›¸å¯¹ä½ç½®ç¼–ç çš„ä½¿ç”¨èŒƒå›´
            relative_scores = self._get_relative_position_scores(seq_len)
            # è°ƒæ•´ç»´åº¦ä»¥åŒ¹é…scores
            relative_scores = relative_scores.unsqueeze(0).unsqueeze(0)  # [1, 1, seq_len, seq_len, head_dim]
            # å–å¹³å‡æˆ–æ±‚å’Œæ¥å‡å°‘ç»´åº¦
            relative_scores = relative_scores.mean(dim=-1)  # [1, 1, seq_len, seq_len]
            scores = scores + relative_scores
        
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        
        attention_weights = F.softmax(scores, dim=-1)
        attention_weights = self.dropout(attention_weights)
        
        # åº”ç”¨æ³¨æ„åŠ›
        out = torch.matmul(attention_weights, v)
        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)
        
        return self.output_projection(out), attention_weights
    
    def _get_relative_position_scores(self, seq_len):
        """è®¡ç®—ç›¸å¯¹ä½ç½®å¾—åˆ†"""
        positions = torch.arange(seq_len, device=self.relative_position_encoding.device)
        relative_positions = positions.unsqueeze(1) - positions.unsqueeze(0)
        
        # è°ƒæ•´åˆ°æ­£æ•°èŒƒå›´
        max_relative_position = (self.relative_position_encoding.size(0) + 1) // 2
        relative_positions = relative_positions + max_relative_position - 1
        
        # ç¡®ä¿ç´¢å¼•åœ¨æœ‰æ•ˆèŒƒå›´å†…
        max_pos = self.relative_position_encoding.size(0) - 1
        relative_positions = torch.clamp(relative_positions, 0, max_pos)
        
        relative_scores = self.relative_position_encoding[relative_positions]
        return relative_scores

class AdvancedFeedForward(nn.Module):
    """å¢å¼ºç‰ˆå‰é¦ˆç½‘ç»œ"""
    def __init__(self, d_model=256, d_ff=1024, dropout=0.1, activation='gelu'):
        super().__init__()
        
        # å¢åŠ ç½‘ç»œæ·±åº¦å’Œå®½åº¦
        self.linear1 = nn.Linear(d_model, d_ff)
        self.linear2 = nn.Linear(d_ff, d_ff // 2)
        self.linear3 = nn.Linear(d_ff // 2, d_model)
        
        # å¤šç§æ¿€æ´»å‡½æ•°é€‰æ‹©
        if activation == 'gelu':
            self.activation = nn.GELU()
        elif activation == 'swish':
            self.activation = nn.SiLU()
        else:
            self.activation = nn.ReLU()
        
        self.dropout = nn.Dropout(dropout)
        self.layer_norm = nn.LayerNorm(d_model)
        
        # é—¨æ§æœºåˆ¶
        self.gate = nn.Linear(d_model, d_ff)
        
    def forward(self, x):
        residual = x
        
        # é—¨æ§å‰é¦ˆç½‘ç»œ
        gate_values = torch.sigmoid(self.gate(x))
        
        # ä¸‰å±‚å‰é¦ˆ
        out = self.linear1(x)
        out = out * gate_values  # é—¨æ§
        out = self.activation(out)
        out = self.dropout(out)
        
        out = self.linear2(out)
        out = self.activation(out)
        out = self.dropout(out)
        
        out = self.linear3(out)
        out = self.dropout(out)
        
        # æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–
        return self.layer_norm(residual + out)

class MultiModalFusion(nn.Module):
    """é«˜çº§å¤šæ¨¡æ€èåˆæ¨¡å—"""
    def __init__(self, 
                 visual_dim=512, 
                 tactile_dim=128, 
                 force_dim=64, 
                 proprioceptive_dim=32,
                 output_dim=256):
        super().__init__()
        
        # å„æ¨¡æ€ç‰¹å¾æå–å™¨
        self.visual_encoder = nn.Sequential(
            nn.Linear(visual_dim, 256),
            nn.LayerNorm(256),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(256, output_dim)
        )
        
        self.tactile_encoder = nn.Sequential(
            # å¤„ç†æ—¶åºè§¦è§‰æ•°æ®ï¼š(batch, seq_len, features) -> (batch, features)
            TemporalAggregator(method='mean'),  # å¯¹æ—¶é—´ç»´åº¦æ±‚å¹³å‡
            nn.Linear(tactile_dim, 128),  # tactile_dimç°åœ¨æ˜¯ç‰¹å¾æ•°é‡
            nn.LayerNorm(128),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(128, output_dim)
        )
        
        self.force_encoder = nn.Sequential(
            # å¤„ç†æ—¶åºåŠ›æ•°æ®ï¼š(batch, seq_len, force_features) -> (batch, force_features)
            TemporalAggregator(method='mean'),  # å¯¹æ—¶é—´ç»´åº¦æ±‚å¹³å‡
            nn.Linear(force_dim, 64),  # force_dimæ˜¯åŠ›çš„ç‰¹å¾æ•°é‡(6)
            nn.LayerNorm(64),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(64, output_dim)
        )
        
        self.proprioceptive_encoder = nn.Sequential(
            # å¤„ç†æ—¶åºå…³èŠ‚æ•°æ®ï¼š(batch, seq_len, joint_features) -> (batch, joint_features)
            TemporalAggregator(method='mean'),  # å¯¹æ—¶é—´ç»´åº¦æ±‚å¹³å‡
            nn.Linear(proprioceptive_dim, 64),  # proprioceptive_dimæ˜¯å…³èŠ‚ç‰¹å¾æ•°é‡(7)
            nn.LayerNorm(64),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(64, output_dim)
        )
        
        # è·¨æ¨¡æ€æ³¨æ„åŠ›
        self.cross_modal_attention = EnhancedMultiHeadAttention(
            d_model=output_dim, 
            num_heads=8
        )
        
        # æ¨¡æ€æƒé‡å­¦ä¹ 
        self.modality_weights = nn.Parameter(torch.ones(4))
        
        # èåˆç½‘ç»œ
        self.fusion_network = nn.Sequential(
            nn.Linear(output_dim * 4, output_dim * 2),
            nn.LayerNorm(output_dim * 2),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(output_dim * 2, output_dim),
            nn.LayerNorm(output_dim)
        )
        
    def forward(self, 
                visual_features, 
                tactile_features, 
                force_features, 
                proprioceptive_features):
        
        # å„æ¨¡æ€ç¼–ç 
        visual_encoded = self.visual_encoder(visual_features)
        tactile_encoded = self.tactile_encoder(tactile_features)
        force_encoded = self.force_encoder(force_features)
        proprioceptive_encoded = self.proprioceptive_encoder(proprioceptive_features)
        
        # å †å æ‰€æœ‰æ¨¡æ€ç‰¹å¾
        all_modalities = torch.stack([
            visual_encoded,
            tactile_encoded,
            force_encoded,
            proprioceptive_encoded
        ], dim=1)  # [batch, 4, output_dim]
        
        # è·¨æ¨¡æ€æ³¨æ„åŠ›
        attended_features, attention_weights = self.cross_modal_attention(all_modalities)
        
        # å­¦ä¹ çš„æ¨¡æ€æƒé‡
        modality_weights = F.softmax(self.modality_weights, dim=0)
        weighted_features = attended_features * modality_weights.view(1, 4, 1)
        
        # èåˆæ‰€æœ‰æ¨¡æ€
        concatenated = weighted_features.flatten(1)  # [batch, 4*output_dim]
        fused_features = self.fusion_network(concatenated)
        
        return fused_features, attention_weights, modality_weights

class EnhancedTaskHead(nn.Module):
    """å¢å¼ºç‰ˆä»»åŠ¡å¤´ - æ”¯æŒ30ä¸ªè¾“å‡ºä»»åŠ¡"""
    def __init__(self, input_dim=256, task_configs=None):
        super().__init__()
        
        if task_configs is None:
            # 30ä¸ªé¤é¥®ç›¸å…³ä»»åŠ¡é…ç½®
            task_configs = {
                # åŸºç¡€æ„ŸçŸ¥ä»»åŠ¡ (1-10)
                'object_position_3d': 3,           # ç‰©ä½“3Dä½ç½®
                'object_orientation': 4,           # ç‰©ä½“å§¿æ€(å››å…ƒæ•°)
                'grasp_point_detection': 3,        # æŠ“å–ç‚¹æ£€æµ‹
                'grasp_quality_score': 1,          # æŠ“å–è´¨é‡è¯„åˆ†
                'object_material_type': 8,         # æè´¨åˆ†ç±»(8ç§å–‚é£Ÿçº¹ç†) - æ‰©å±•åˆ°8ä¸ªå–‚é£Ÿåœºæ™¯ç±»åˆ«
                'object_temperature': 1,           # æ¸©åº¦ä¼°è®¡
                'object_weight': 1,                # é‡é‡é¢„æµ‹
                'object_fragility': 1,             # è„†æ€§ç¨‹åº¦
                'slip_risk_assessment': 1,         # æ»‘ç§»é£é™©
                'collision_detection': 1,          # ç¢°æ’æ£€æµ‹
                
                # åŠ¨ä½œæ§åˆ¶ä»»åŠ¡ (11-20)
                'trajectory_planning': 7,          # è½¨è¿¹è§„åˆ’(7DOF)
                'velocity_control': 7,             # é€Ÿåº¦æ§åˆ¶
                'force_control': 3,                # åŠ›æ§åˆ¶(3è½´)
                'gripper_control': 2,              # å¤¹çˆªæ§åˆ¶(å¼€åˆ+åŠ›åº¦)
                'motion_smoothness': 1,            # è¿åŠ¨å¹³æ»‘åº¦
                'precision_requirement': 1,        # ç²¾åº¦è¦æ±‚
                'safety_margin': 1,                # å®‰å…¨è¾¹è·
                'emergency_stop_trigger': 1,       # ç´§æ€¥åœæ­¢
                'approach_strategy': 5,            # æ¥è¿‘ç­–ç•¥(5ç§)
                'retreat_strategy': 5,             # æ’¤é€€ç­–ç•¥(5ç§)
                
                # é«˜çº§é¤é¥®ä»»åŠ¡ (21-30)
                'food_recognition': 20,            # é£Ÿç‰©è¯†åˆ«(20ç±»)
                'portion_estimation': 1,           # åˆ†é‡ä¼°è®¡
                'feeding_sequence': 10,            # è¿›é£Ÿåºåˆ—(10æ­¥)
                'utensil_selection': 5,            # é¤å…·é€‰æ‹©(5ç§)
                'serving_technique': 8,            # æœåŠ¡æŠ€å·§(8ç§)
                'spillage_prevention': 1,          # é˜²æº…ç­–ç•¥
                'user_preference': 10,             # ç”¨æˆ·åå¥½(10ç»´)
                'dietary_restriction': 15,         # é¥®é£Ÿé™åˆ¶(15ç§)
                'meal_timing': 1,                  # ç”¨é¤æ—¶æœº
                'satisfaction_prediction': 1       # æ»¡æ„åº¦é¢„æµ‹
            }
        
        self.task_configs = task_configs
        self.num_tasks = len(task_configs)
        
        # 8ä¸ªå–‚é£Ÿåœºæ™¯çº¹ç†ç±»åˆ«æ˜ å°„
        self.feeding_texture_classes = {
            0: 'liquid_food',      # æ¶²ä½“é£Ÿç‰© (å¦‚æ±¤ã€ç²¥ã€æœæ±)
            1: 'solid_food',       # å›ºä½“é£Ÿç‰© (å¦‚é¥¼å¹²ã€é¢åŒ…ã€è‹¹æœ)
            2: 'soft_food',        # è½¯è´¨é£Ÿç‰© (å¦‚é¦™è•‰ã€è’¸è›‹ã€è±†è…)
            3: 'hard_food',        # ç¡¬è´¨é£Ÿç‰© (å¦‚åšæœã€èƒ¡èåœã€é¥¼å¹²)
            4: 'sticky_food',      # ç²˜æ€§é£Ÿç‰© (å¦‚èœ‚èœœã€é…¸å¥¶ã€æœé…±)
            5: 'slippery_food',    # å…‰æ»‘æ˜“æ»‘é£Ÿç‰© (å¦‚æœå†»ã€å¸ƒä¸ã€é¦™è•‰)
            6: 'crumbly_food',     # æ˜“ç¢é£Ÿç‰© (å¦‚é¥¼å¹²å±‘ã€è›‹ç³•ã€é…¥è„†é›¶é£Ÿ)
            7: 'wet_food'          # æ¹¿æ¶¦é£Ÿç‰© (å¦‚æ°´æœã€æ±¤å“ã€æ¹¿æ¶¦è›‹ç³•)
        }
        
        # å…±äº«ç‰¹å¾æå–å™¨
        self.shared_encoder = nn.Sequential(
            nn.Linear(input_dim, 512),
            nn.LayerNorm(512),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(512, 384),
            nn.LayerNorm(384),
            nn.GELU(),
            nn.Dropout(0.1)
        )
        
        # ä¸ºæ¯ä¸ªä»»åŠ¡åˆ›å»ºä¸“ç”¨å¤´
        self.task_heads = nn.ModuleDict()
        for task_name, output_dim in task_configs.items():
            self.task_heads[task_name] = nn.Sequential(
                nn.Linear(384, 128),
                nn.LayerNorm(128),
                nn.GELU(),
                nn.Dropout(0.1),
                nn.Linear(128, 64),
                nn.GELU(),
                nn.Linear(64, output_dim)
            )
        
        # ä»»åŠ¡é—´ä¾èµ–å…³ç³»å»ºæ¨¡
        self.task_interaction = nn.MultiheadAttention(
            embed_dim=384, 
            num_heads=12, 
            batch_first=True
        )
        
    def forward(self, x):
        # å…±äº«ç‰¹å¾æå–
        shared_features = self.shared_encoder(x)  # [batch, 384]
        
        # ä¸ºä»»åŠ¡é—´äº¤äº’å‡†å¤‡ç‰¹å¾
        batch_size = shared_features.size(0)
        task_features = shared_features.unsqueeze(1).repeat(1, self.num_tasks, 1)
        
        # ä»»åŠ¡é—´æ³¨æ„åŠ›
        attended_features, _ = self.task_interaction(
            task_features, task_features, task_features
        )
        
        # å„ä»»åŠ¡é¢„æµ‹
        task_outputs = {}
        for i, (task_name, head) in enumerate(self.task_heads.items()):
            task_specific_features = attended_features[:, i, :]
            task_outputs[task_name] = head(task_specific_features)
        
        return task_outputs

class UncertaintyQuantification(nn.Module):
    """å¢å¼ºç‰ˆä¸ç¡®å®šæ€§é‡åŒ–æ¨¡å—"""
    def __init__(self, input_dim=256, num_monte_carlo=10):
        super().__init__()
        self.num_monte_carlo = num_monte_carlo
        
        # è´å¶æ–¯ç¥ç»ç½‘ç»œå±‚
        self.bayesian_layers = nn.ModuleList([
            nn.Sequential(
                nn.Linear(input_dim, 128),
                nn.Dropout(0.2),  # ç”¨äºMonte Carlo Dropout
                nn.GELU(),
                nn.Linear(128, 64),
                nn.Dropout(0.2),
                nn.GELU(),
                nn.Linear(64, 1)
            ) for _ in range(5)  # 5ä¸ªä¸åŒçš„ä¸ç¡®å®šæ€§ä¼°è®¡å™¨
        ])
        
        # ä¸ç¡®å®šæ€§èåˆ
        self.uncertainty_fusion = nn.Sequential(
            nn.Linear(5, 16),
            nn.GELU(),
            nn.Linear(16, 1),
            nn.Sigmoid()
        )
        
    def forward(self, x, training=True):
        if training:
            # è®­ç»ƒæ—¶ä½¿ç”¨Monte Carlo Dropout
            uncertainties = []
            for layer in self.bayesian_layers:
                mc_samples = []
                for _ in range(self.num_monte_carlo):
                    mc_samples.append(layer(x))
                
                # è®¡ç®—æ–¹å·®ä½œä¸ºä¸ç¡®å®šæ€§
                mc_tensor = torch.stack(mc_samples, dim=0)
                uncertainty = torch.var(mc_tensor, dim=0)
                uncertainties.append(uncertainty)
            
            uncertainties = torch.cat(uncertainties, dim=-1)
            
        else:
            # æ¨ç†æ—¶ç›´æ¥è®¡ç®—
            uncertainties = []
            for layer in self.bayesian_layers:
                uncertainty = layer(x)
                uncertainties.append(uncertainty)
            uncertainties = torch.cat(uncertainties, dim=-1)
        
        # èåˆä¸ç¡®å®šæ€§
        final_uncertainty = self.uncertainty_fusion(uncertainties)
        
        return final_uncertainty

class AdaLeRobotEnhancedPerceptionModel(nn.Module):
    """
    Ada LeRobotå¢å¼ºç‰ˆæ„ŸçŸ¥æ¨¡å‹ - 10Må‚æ•°ç‰ˆæœ¬
    
    å¢å¼ºç‰¹æ€§:
    - 30ä¸ªè¾“å‡ºä»»åŠ¡ (ä»18ä¸ªæ‰©å±•)
    - æ›´æ·±çš„Transformeræ¶æ„ (8å±‚ -> 12å±‚)
    - å¢å¼ºçš„å¤šæ¨¡æ€èåˆ
    - æ›´ç²¾ç»†çš„æ³¨æ„åŠ›æœºåˆ¶
    - é«˜çº§ä¸ç¡®å®šæ€§é‡åŒ–
    - ä»»åŠ¡é—´ä¾èµ–å…³ç³»å»ºæ¨¡
    """
    
    def __init__(self,
                 visual_dim=512,
                 tactile_dim=128,
                 force_dim=64,
                 proprioceptive_dim=32,
                 d_model=256,
                 num_layers=12,  # å¢åŠ åˆ°12å±‚
                 num_heads=16,   # å¢åŠ å¤´æ•°
                 dropout=0.1,
                 task_configs=None):
        super().__init__()
        
        self.d_model = d_model
        self.num_layers = num_layers
        
        # å¤šæ¨¡æ€èåˆæ¨¡å—
        self.multimodal_fusion = MultiModalFusion(
            visual_dim=visual_dim,
            tactile_dim=tactile_dim,
            force_dim=force_dim,
            proprioceptive_dim=proprioceptive_dim,
            output_dim=d_model
        )
        
        # ä½ç½®ç¼–ç 
        self.positional_encoding = nn.Parameter(torch.randn(1, 512, d_model))
        
        # å¢å¼ºçš„Transformerå±‚
        self.transformer_layers = nn.ModuleList([
            nn.ModuleDict({
                'attention': EnhancedMultiHeadAttention(
                    d_model=d_model, 
                    num_heads=num_heads, 
                    dropout=dropout
                ),
                'feedforward': AdvancedFeedForward(
                    d_model=d_model, 
                    d_ff=d_model*4, 
                    dropout=dropout,
                    activation='gelu'
                ),
                'norm1': nn.LayerNorm(d_model),
                'norm2': nn.LayerNorm(d_model)
            }) for _ in range(num_layers)
        ])
        
        # å¢å¼ºä»»åŠ¡å¤´ (30ä¸ªä»»åŠ¡)
        self.task_head = EnhancedTaskHead(input_dim=d_model, task_configs=task_configs)
        
        # ä¸ç¡®å®šæ€§é‡åŒ–
        self.uncertainty_quantification = UncertaintyQuantification(
            input_dim=d_model, 
            num_monte_carlo=10
        )
        
        # å…¨å±€ç‰¹å¾èšåˆ
        self.global_aggregation = nn.Sequential(
            nn.AdaptiveAvgPool1d(1),
            nn.Flatten(),
            nn.Linear(d_model, d_model),
            nn.LayerNorm(d_model),
            nn.GELU()
        )
        
        # åˆå§‹åŒ–æƒé‡
        self._initialize_weights()
    
    def _initialize_weights(self):
        """åˆå§‹åŒ–æ¨¡å‹æƒé‡"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
            elif isinstance(module, nn.LayerNorm):
                nn.init.ones_(module.weight)
                nn.init.zeros_(module.bias)
    
    def forward(self, 
                visual_features,
                tactile_features, 
                force_features,
                proprioceptive_features,
                return_attention=False,
                return_uncertainty=True):
        
        batch_size = visual_features.size(0)
        
        # å¤šæ¨¡æ€èåˆ
        fused_features, modal_attention, modal_weights = self.multimodal_fusion(
            visual_features, tactile_features, force_features, proprioceptive_features
        )
        
        # æ·»åŠ ç»´åº¦ä»¥é€‚åº”Transformer
        x = fused_features.unsqueeze(1)  # [batch, 1, d_model]
        
        # æ·»åŠ ä½ç½®ç¼–ç 
        seq_len = x.size(1)
        x = x + self.positional_encoding[:, :seq_len, :]
        
        # Transformerå¤„ç†
        attention_weights = []
        for layer in self.transformer_layers:
            # å¤šå¤´æ³¨æ„åŠ›
            attn_out, attn_weights = layer['attention'](x)
            x = layer['norm1'](x + attn_out)
            
            # å‰é¦ˆç½‘ç»œ
            ff_out = layer['feedforward'](x)
            x = layer['norm2'](x + ff_out)
            
            if return_attention:
                attention_weights.append(attn_weights)
        
        # å…¨å±€ç‰¹å¾èšåˆ
        global_features = self.global_aggregation(x.transpose(1, 2))
        
        # ä»»åŠ¡é¢„æµ‹
        task_outputs = self.task_head(global_features)
        
        # ä¸ç¡®å®šæ€§é‡åŒ–
        uncertainty = None
        if return_uncertainty:
            uncertainty = self.uncertainty_quantification(
                global_features, 
                training=self.training
            )
        
        outputs = {
            'task_outputs': task_outputs,
            'global_features': global_features,
            'modal_attention': modal_attention,
            'modal_weights': modal_weights
        }
        
        if return_attention:
            outputs['attention_weights'] = attention_weights
            
        if return_uncertainty:
            outputs['uncertainty'] = uncertainty
        
        return outputs
    
    def get_model_stats(self):
        """è·å–æ¨¡å‹ç»Ÿè®¡ä¿¡æ¯"""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        
        # å„ç»„ä»¶å‚æ•°é‡
        component_params = {
            'multimodal_fusion': sum(p.numel() for p in self.multimodal_fusion.parameters()),
            'transformer_layers': sum(p.numel() for p in self.transformer_layers.parameters()),
            'task_head': sum(p.numel() for p in self.task_head.parameters()),
            'uncertainty_quantification': sum(p.numel() for p in self.uncertainty_quantification.parameters()),
            'global_aggregation': sum(p.numel() for p in self.global_aggregation.parameters())
        }
        
        return {
            'total_parameters': total_params,
            'trainable_parameters': trainable_params,
            'component_parameters': component_params,
            'model_size_mb': total_params * 4 / (1024 * 1024),  # å‡è®¾float32
            'num_tasks': len(self.task_head.task_configs),
            'transformer_layers': self.num_layers,
            'model_dimension': self.d_model
        }

def create_enhanced_ada_lerobot_model():
    """åˆ›å»ºå¢å¼ºç‰ˆAda LeRobotæ„ŸçŸ¥æ¨¡å‹"""
    model = AdaLeRobotEnhancedPerceptionModel(
        visual_dim=512,      # è§†è§‰ç‰¹å¾ç»´åº¦
        tactile_dim=128,     # è§¦è§‰ç‰¹å¾ç»´åº¦  
        force_dim=64,        # åŠ›è§‰ç‰¹å¾ç»´åº¦
        proprioceptive_dim=32, # æœ¬ä½“æ„Ÿè§‰ç»´åº¦
        d_model=256,         # æ¨¡å‹ç»´åº¦
        num_layers=12,       # Transformerå±‚æ•°
        num_heads=16,        # æ³¨æ„åŠ›å¤´æ•°
        dropout=0.1
    )
    
    return model

def demo_enhanced_model():
    """æ¼”ç¤ºå¢å¼ºç‰ˆæ¨¡å‹"""
    print("ğŸš€ Ada LeRobotå¢å¼ºç‰ˆæ„ŸçŸ¥æ¨¡å‹ v2.0")
    print("=" * 60)
    
    # åˆ›å»ºæ¨¡å‹
    model = create_enhanced_ada_lerobot_model()
    model.eval()
    
    # è·å–æ¨¡å‹ç»Ÿè®¡ä¿¡æ¯
    stats = model.get_model_stats()
    
    print(f"ğŸ“Š æ¨¡å‹è§„æ¨¡ç»Ÿè®¡:")
    print(f"  æ€»å‚æ•°é‡: {stats['total_parameters']:,} ({stats['total_parameters']/1000000:.2f}M)")
    print(f"  å¯è®­ç»ƒå‚æ•°: {stats['trainable_parameters']:,}")
    print(f"  æ¨¡å‹å¤§å°: {stats['model_size_mb']:.2f} MB")
    print(f"  è¾“å‡ºä»»åŠ¡æ•°: {stats['num_tasks']}")
    print(f"  Transformerå±‚æ•°: {stats['transformer_layers']}")
    print(f"  æ¨¡å‹ç»´åº¦: {stats['model_dimension']}")
    
    print(f"\nğŸ”§ å„ç»„ä»¶å‚æ•°åˆ†å¸ƒ:")
    for component, params in stats['component_parameters'].items():
        percentage = (params / stats['total_parameters']) * 100
        print(f"  {component}: {params:,} ({percentage:.1f}%)")
    
    # æ¨¡æ‹Ÿè¾“å…¥
    batch_size = 4
    visual_features = torch.randn(batch_size, 512)
    tactile_features = torch.randn(batch_size, 128)
    force_features = torch.randn(batch_size, 64)
    proprioceptive_features = torch.randn(batch_size, 32)
    
    print(f"\nğŸ§ª æ¨¡å‹æ¨ç†æµ‹è¯•:")
    print(f"  æ‰¹æ¬¡å¤§å°: {batch_size}")
    
    # å‰å‘ä¼ æ’­
    with torch.no_grad():
        outputs = model(
            visual_features=visual_features,
            tactile_features=tactile_features,
            force_features=force_features,
            proprioceptive_features=proprioceptive_features,
            return_attention=True,
            return_uncertainty=True
        )
    
    print(f"âœ… æ¨ç†æˆåŠŸ!")
    print(f"  ä»»åŠ¡è¾“å‡º: {len(outputs['task_outputs'])} ä¸ªä»»åŠ¡")
    print(f"  å…¨å±€ç‰¹å¾ç»´åº¦: {outputs['global_features'].shape}")
    print(f"  æ¨¡æ€æ³¨æ„åŠ›æƒé‡: {outputs['modal_weights']}")
    print(f"  ä¸ç¡®å®šæ€§å¾—åˆ†: {outputs['uncertainty'].mean().item():.4f}")
    
    # æ˜¾ç¤º30ä¸ªä»»åŠ¡çš„è¾“å‡º
    print(f"\nğŸ“‹ 30ä¸ªé¤é¥®ä»»åŠ¡è¾“å‡º:")
    for i, (task_name, task_output) in enumerate(outputs['task_outputs'].items(), 1):
        print(f"  {i:2d}. {task_name}: {task_output.shape[1]} ç»´è¾“å‡º")
    
    print(f"\nğŸ¯ æ¨¡å‹ä¼˜åŠ¿:")
    print(f"  âœ… å‚æ•°é‡æå‡åˆ° {stats['total_parameters']/1000000:.1f}M")
    print(f"  âœ… 30ä¸ªé¤é¥®ä¸“ä¸šä»»åŠ¡")
    print(f"  âœ… 4ç§æ¨¡æ€æ·±åº¦èåˆ")
    print(f"  âœ… 12å±‚Transformeræ¶æ„")
    print(f"  âœ… é«˜çº§ä¸ç¡®å®šæ€§é‡åŒ–")
    print(f"  âœ… ä»»åŠ¡é—´ä¾èµ–å…³ç³»å»ºæ¨¡")
    
    return model, stats

if __name__ == "__main__":
    model, stats = demo_enhanced_model()
