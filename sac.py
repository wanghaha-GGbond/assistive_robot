#!/usr/bin/env python3
"""
SACå¼ºåŒ–å­¦ä¹ è®­ç»ƒå™¨
åŸºäºcheckpoint_integrated_supervised_epoch_80.pthè¿›è¡Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒ

æ¨¡å‹æ¶æ„: Integrated VLA-SAC
è®­ç»ƒæ–¹æ³•: Soft Actor-Critic (SAC)
æ•°æ®é›†: Subject1-9åŠ©é¤æ•°æ®é›†
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
import random
import logging
from collections import deque
from pathlib import Path
from dataclasses import dataclass
from typing import Dict, Tuple, List, Optional
import json
import time
from datetime import datetime

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class SACConfig:
    """SACå¼ºåŒ–å­¦ä¹ é…ç½® - åŸºäºå®é™…æ¨¡å‹é…ç½®"""
    
    # æ¨¡å‹è·¯å¾„
    supervised_model_path: str = "checkpoint_integrated_supervised_epoch_80.pth"
    output_dir: str = "outputs/sac_reinforcement_learning"
    
    # SACè¶…å‚æ•° (ä»å®é™…é…ç½®æå–)
    sac_lr: float = 0.0003
    tau: float = 0.005
    alpha: float = 0.2
    auto_alpha: bool = True
    target_entropy: float = -7.0
    gamma: float = 0.99
    
    # ç»éªŒå›æ”¾
    replay_buffer_size: int = 50000
    min_replay_size: int = 500
    batch_size: int = 256
    
    # è®­ç»ƒé…ç½®
    rl_epochs: int = 40
    steps_per_epoch: int = 1000
    eval_episodes: int = 10
    save_every: int = 5
    
    # ç¯å¢ƒé…ç½®
    action_dim: int = 7
    state_dim: int = 256  # æ„ŸçŸ¥æ¨¡å—è¾“å‡ºç»´åº¦
    max_episode_steps: int = 100
    
    # ç½‘ç»œé…ç½®
    hidden_dim: int = 256
    num_layers: int = 2

class ReplayBuffer:
    """ç»éªŒå›æ”¾ç¼“å†²åŒº"""
    
    def __init__(self, capacity: int):
        self.buffer = deque(maxlen=capacity)
    
    def push(self, state, action, reward, next_state, done):
        """æ·»åŠ ç»éªŒ"""
        self.buffer.append((state, action, reward, next_state, done))
    
    def sample(self, batch_size: int):
        """é‡‡æ ·æ‰¹æ¬¡æ•°æ®"""
        batch = random.sample(self.buffer, batch_size)
        state, action, reward, next_state, done = map(np.stack, zip(*batch))
        return (
            torch.FloatTensor(state),
            torch.FloatTensor(action),
            torch.FloatTensor(reward).unsqueeze(1),
            torch.FloatTensor(next_state),
            torch.FloatTensor(done).unsqueeze(1)
        )
    
    def __len__(self):
        return len(self.buffer)

class Actor(nn.Module):
    """SAC Actorç½‘ç»œ (ç­–ç•¥ç½‘ç»œ)"""
    
    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 256):
        super().__init__()
        
        self.net = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU()
        )
        
        self.mean_head = nn.Linear(hidden_dim, action_dim)
        self.log_std_head = nn.Linear(hidden_dim, action_dim)
        
        # åŠ¨ä½œèŒƒå›´
        self.action_scale = 1.0
        self.action_bias = 0.0
        
    def forward(self, state):
        x = self.net(state)
        mean = self.mean_head(x)
        log_std = self.log_std_head(x)
        log_std = torch.clamp(log_std, min=-20, max=2)
        return mean, log_std
    
    def sample(self, state):
        mean, log_std = self.forward(state)
        std = log_std.exp()
        normal = torch.distributions.Normal(mean, std)
        x_t = normal.rsample()  # é‡å‚æ•°åŒ–é‡‡æ ·
        y_t = torch.tanh(x_t)
        action = y_t * self.action_scale + self.action_bias
        log_prob = normal.log_prob(x_t)
        # ä¿®æ­£tanhå˜æ¢çš„log_prob
        log_prob -= torch.log(self.action_scale * (1 - y_t.pow(2)) + 1e-6)
        log_prob = log_prob.sum(1, keepdim=True)
        mean = torch.tanh(mean) * self.action_scale + self.action_bias
        return action, log_prob, mean

class Critic(nn.Module):
    """SAC Criticç½‘ç»œ (Qç½‘ç»œ)"""
    
    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 256):
        super().__init__()
        
        # Q1ç½‘ç»œ
        self.q1 = nn.Sequential(
            nn.Linear(state_dim + action_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )
        
        # Q2ç½‘ç»œ
        self.q2 = nn.Sequential(
            nn.Linear(state_dim + action_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )
    
    def forward(self, state, action):
        sa = torch.cat([state, action], 1)
        q1 = self.q1(sa)
        q2 = self.q2(sa)
        return q1, q2

class FeedingEnvironment:
    """åŠ©é¤ç¯å¢ƒæ¨¡æ‹Ÿå™¨"""
    
    def __init__(self, config: SACConfig):
        self.config = config
        self.max_steps = config.max_episode_steps
        self.current_step = 0
        self.state_dim = config.state_dim
        self.action_dim = config.action_dim
        
        # ç¯å¢ƒçŠ¶æ€
        self.current_state = None
        self.target_position = None
        self.food_type = None
        self.subject_id = None
        
    def reset(self):
        """é‡ç½®ç¯å¢ƒ"""
        self.current_step = 0
        
        # éšæœºé€‰æ‹©Subjectå’Œé£Ÿç‰©ç±»å‹
        self.subject_id = np.random.randint(1, 10)  # Subject 1-9
        self.food_type = np.random.randint(0, 9)    # 9ç§é£Ÿç‰©
        
        # ç”Ÿæˆåˆå§‹çŠ¶æ€ (æ¨¡æ‹Ÿå¤šæ¨¡æ€æ„ŸçŸ¥è¾“å‡º)
        self.current_state = self._generate_state()
        self.target_position = np.random.uniform(-1, 1, self.action_dim)
        
        return self.current_state
    
    def step(self, action):
        """æ‰§è¡ŒåŠ¨ä½œ"""
        self.current_step += 1
        
        # è®¡ç®—å¥–åŠ±
        reward = self._calculate_reward(action)
        
        # æ›´æ–°çŠ¶æ€
        next_state = self._generate_state()
        
        # åˆ¤æ–­æ˜¯å¦ç»“æŸ
        done = self.current_step >= self.max_steps or self._is_success(action)
        
        self.current_state = next_state
        
        return next_state, reward, done, {}
    
    def _generate_state(self):
        """ç”ŸæˆçŠ¶æ€ (æ¨¡æ‹Ÿæ„ŸçŸ¥æ¨¡å—è¾“å‡º)"""
        # æ¨¡æ‹Ÿå¤šæ¨¡æ€ç‰¹å¾èåˆåçš„256ç»´çŠ¶æ€
        state = np.random.randn(self.state_dim).astype(np.float32)
        
        # æ·»åŠ Subjectå’Œé£Ÿç‰©ç±»å‹ä¿¡æ¯
        state[0] = self.subject_id / 10.0  # å½’ä¸€åŒ–Subject ID
        state[1] = self.food_type / 9.0    # å½’ä¸€åŒ–é£Ÿç‰©ç±»å‹
        
        return state
    
    def _calculate_reward(self, action):
        """è®¡ç®—å¥–åŠ±å‡½æ•°"""
        # åŸºç¡€å¥–åŠ±ï¼šåŠ¨ä½œä¸ç›®æ ‡ä½ç½®çš„è·ç¦»
        distance_reward = -np.linalg.norm(action - self.target_position)
        
        # å®‰å…¨å¥–åŠ±ï¼šé¿å…è¿‡å¤§çš„åŠ¨ä½œ
        safety_reward = -0.1 * np.sum(np.abs(action))
        
        # å¹³æ»‘å¥–åŠ±ï¼šé¿å…å‰§çƒˆå˜åŒ–
        smoothness_reward = -0.05 * np.sum(action ** 2)
        
        # ä»»åŠ¡å®Œæˆå¥–åŠ±
        success_reward = 10.0 if self._is_success(action) else 0.0
        
        # é£Ÿç‰©ç‰¹å®šå¥–åŠ± (ä¸åŒé£Ÿç‰©æœ‰ä¸åŒéš¾åº¦)
        food_difficulty = [1.0, 1.2, 1.1, 1.0, 1.5, 1.3, 1.4, 1.6, 1.2]  # å¯¹åº”9ç§é£Ÿç‰©
        difficulty_factor = food_difficulty[self.food_type]
        
        total_reward = (distance_reward + safety_reward + smoothness_reward + success_reward) / difficulty_factor
        
        return total_reward
    
    def _is_success(self, action):
        """åˆ¤æ–­æ˜¯å¦æˆåŠŸ"""
        distance = np.linalg.norm(action - self.target_position)
        return distance < 0.1  # æˆåŠŸé˜ˆå€¼

class SACAgent:
    """SACæ™ºèƒ½ä½“"""
    
    def __init__(self, config: SACConfig, device: torch.device):
        self.config = config
        self.device = device
        
        # ç½‘ç»œåˆå§‹åŒ–
        self.actor = Actor(config.state_dim, config.action_dim, config.hidden_dim).to(device)
        self.critic = Critic(config.state_dim, config.action_dim, config.hidden_dim).to(device)
        self.critic_target = Critic(config.state_dim, config.action_dim, config.hidden_dim).to(device)
        
        # å¤åˆ¶å‚æ•°åˆ°ç›®æ ‡ç½‘ç»œ
        self.critic_target.load_state_dict(self.critic.state_dict())
        
        # ä¼˜åŒ–å™¨
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=config.sac_lr)
        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=config.sac_lr)
        
        # è‡ªåŠ¨è°ƒæ•´ç†µç³»æ•°
        if config.auto_alpha:
            self.target_entropy = config.target_entropy
            self.log_alpha = torch.zeros(1, requires_grad=True, device=device)
            self.alpha_optimizer = optim.Adam([self.log_alpha], lr=config.sac_lr)
        else:
            self.alpha = config.alpha
        
        # ç»éªŒå›æ”¾
        self.replay_buffer = ReplayBuffer(config.replay_buffer_size)
        
        # è®­ç»ƒç»Ÿè®¡
        self.total_steps = 0
        self.episode_rewards = []
        self.losses = {'actor': [], 'critic': [], 'alpha': []}
        
    @property
    def alpha(self):
        if self.config.auto_alpha:
            return self.log_alpha.exp()
        else:
            return self.config.alpha
    
    def select_action(self, state, evaluate=False):
        """é€‰æ‹©åŠ¨ä½œ"""
        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        
        if evaluate:
            _, _, action = self.actor.sample(state)
        else:
            action, _, _ = self.actor.sample(state)
        
        return action.detach().cpu().numpy()[0]
    
    def update(self):
        """æ›´æ–°ç½‘ç»œå‚æ•°"""
        if len(self.replay_buffer) < self.config.min_replay_size:
            return
        
        # é‡‡æ ·æ‰¹æ¬¡æ•°æ®
        state, action, reward, next_state, done = self.replay_buffer.sample(self.config.batch_size)
        state = state.to(self.device)
        action = action.to(self.device)
        reward = reward.to(self.device)
        next_state = next_state.to(self.device)
        done = done.to(self.device)
        
        # æ›´æ–°Critic
        with torch.no_grad():
            next_action, next_log_prob, _ = self.actor.sample(next_state)
            q1_next, q2_next = self.critic_target(next_state, next_action)
            q_next = torch.min(q1_next, q2_next) - self.alpha * next_log_prob
            q_target = reward + (1 - done) * self.config.gamma * q_next
        
        q1, q2 = self.critic(state, action)
        critic_loss = F.mse_loss(q1, q_target) + F.mse_loss(q2, q_target)
        
        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        self.critic_optimizer.step()
        
        # æ›´æ–°Actor
        new_action, log_prob, _ = self.actor.sample(state)
        q1_new, q2_new = self.critic(state, new_action)
        q_new = torch.min(q1_new, q2_new)
        actor_loss = (self.alpha * log_prob - q_new).mean()
        
        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        self.actor_optimizer.step()
        
        # æ›´æ–°Alpha (å¦‚æœè‡ªåŠ¨è°ƒæ•´)
        alpha_loss = 0
        if self.config.auto_alpha:
            alpha_loss = -(self.log_alpha * (log_prob + self.target_entropy).detach()).mean()
            self.alpha_optimizer.zero_grad()
            alpha_loss.backward()
            self.alpha_optimizer.step()
        
        # è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ
        for target_param, param in zip(self.critic_target.parameters(), self.critic.parameters()):
            target_param.data.copy_(self.config.tau * param.data + (1 - self.config.tau) * target_param.data)
        
        # è®°å½•æŸå¤±
        self.losses['critic'].append(critic_loss.item())
        self.losses['actor'].append(actor_loss.item())
        if self.config.auto_alpha:
            self.losses['alpha'].append(alpha_loss.item())

class SACTrainer:
    """SACè®­ç»ƒå™¨"""
    
    def __init__(self, config: SACConfig):
        self.config = config
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        Path(config.output_dir).mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–ç¯å¢ƒå’Œæ™ºèƒ½ä½“
        self.env = FeedingEnvironment(config)
        self.agent = SACAgent(config, self.device)
        
        # åŠ è½½ç›‘ç£å­¦ä¹ æ¨¡å‹ (å¦‚æœå­˜åœ¨)
        self.load_supervised_model()
        
        # è®­ç»ƒç»Ÿè®¡
        self.training_stats = {
            'episode_rewards': [],
            'success_rates': [],
            'losses': {'actor': [], 'critic': [], 'alpha': []},
            'evaluation_scores': []
        }
        
        logger.info(f"ğŸš€ SACè®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"   è®¾å¤‡: {self.device}")
        logger.info(f"   è¾“å‡ºç›®å½•: {config.output_dir}")
        logger.info(f"   å¼ºåŒ–å­¦ä¹ è½®æ•°: {config.rl_epochs}")
    
    def load_supervised_model(self):
        """åŠ è½½ç›‘ç£å­¦ä¹ æ¨¡å‹ä½œä¸ºåˆå§‹åŒ–"""
        if Path(self.config.supervised_model_path).exists():
            try:
                checkpoint = torch.load(self.config.supervised_model_path, map_location=self.device)
                logger.info(f"âœ… æˆåŠŸåŠ è½½ç›‘ç£å­¦ä¹ æ¨¡å‹: {self.config.supervised_model_path}")
                logger.info(f"   ç›‘ç£å­¦ä¹ è½®æ•°: {checkpoint.get('epoch', 'Unknown')}")
                logger.info(f"   ç›‘ç£å­¦ä¹ æŸå¤±: {checkpoint.get('metric', 'Unknown')}")
                
                # è¿™é‡Œå¯ä»¥æå–é¢„è®­ç»ƒçš„ç‰¹å¾æå–å™¨æƒé‡
                # ç”±äºæˆ‘ä»¬ä½¿ç”¨ç®€åŒ–çš„SACç½‘ç»œï¼Œæš‚æ—¶ä¸ç›´æ¥åŠ è½½æƒé‡
                
            except Exception as e:
                logger.warning(f"âš ï¸ æ— æ³•åŠ è½½ç›‘ç£å­¦ä¹ æ¨¡å‹: {e}")
        else:
            logger.warning(f"âš ï¸ ç›‘ç£å­¦ä¹ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {self.config.supervised_model_path}")
    
    def train(self):
        """å¼€å§‹SACè®­ç»ƒ"""
        logger.info("ğŸ¯ å¼€å§‹SACå¼ºåŒ–å­¦ä¹ è®­ç»ƒ")
        
        total_steps = 0
        
        for epoch in range(self.config.rl_epochs):
            epoch_rewards = []
            epoch_successes = 0
            
            for episode in range(self.config.steps_per_epoch // self.config.max_episode_steps):
                # é‡ç½®ç¯å¢ƒ
                state = self.env.reset()
                episode_reward = 0
                episode_success = False
                
                for step in range(self.config.max_episode_steps):
                    # é€‰æ‹©åŠ¨ä½œ
                    action = self.agent.select_action(state)
                    
                    # æ‰§è¡ŒåŠ¨ä½œ
                    next_state, reward, done, info = self.env.step(action)
                    
                    # å­˜å‚¨ç»éªŒ
                    self.agent.replay_buffer.push(state, action, reward, next_state, done)
                    
                    # æ›´æ–°ç½‘ç»œ
                    if total_steps > self.config.min_replay_size:
                        self.agent.update()
                    
                    episode_reward += reward
                    state = next_state
                    total_steps += 1
                    
                    if done:
                        if reward > 5.0:  # æˆåŠŸå¥–åŠ±é˜ˆå€¼
                            episode_success = True
                        break
                
                epoch_rewards.append(episode_reward)
                if episode_success:
                    epoch_successes += 1
            
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            avg_reward = np.mean(epoch_rewards)
            success_rate = epoch_successes / len(epoch_rewards)
            
            # è¯„ä¼°
            eval_score = self.evaluate()
            
            # è®°å½•ç»Ÿè®¡
            self.training_stats['episode_rewards'].extend(epoch_rewards)
            self.training_stats['success_rates'].append(success_rate)
            self.training_stats['evaluation_scores'].append(eval_score)
            
            # ä¿å­˜æ¨¡å‹
            if (epoch + 1) % self.config.save_every == 0:
                self.save_checkpoint(epoch)
            
            logger.info(f"Epoch {epoch+1}/{self.config.rl_epochs}: "
                       f"Avg Reward: {avg_reward:.3f}, "
                       f"Success Rate: {success_rate:.3f}, "
                       f"Eval Score: {eval_score:.3f}, "
                       f"Alpha: {self.agent.alpha:.3f}")
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        self.save_checkpoint(self.config.rl_epochs - 1, is_final=True)
        self.save_training_report()
        
        logger.info("âœ… SACå¼ºåŒ–å­¦ä¹ è®­ç»ƒå®Œæˆ!")
    
    def evaluate(self):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        eval_rewards = []
        
        for _ in range(self.config.eval_episodes):
            state = self.env.reset()
            episode_reward = 0
            
            for _ in range(self.config.max_episode_steps):
                action = self.agent.select_action(state, evaluate=True)
                next_state, reward, done, _ = self.env.step(action)
                episode_reward += reward
                state = next_state
                
                if done:
                    break
            
            eval_rewards.append(episode_reward)
        
        return np.mean(eval_rewards)
    
    def save_checkpoint(self, epoch: int, is_final: bool = False):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': epoch,
            'actor_state_dict': self.agent.actor.state_dict(),
            'critic_state_dict': self.agent.critic.state_dict(),
            'critic_target_state_dict': self.agent.critic_target.state_dict(),
            'actor_optimizer_state_dict': self.agent.actor_optimizer.state_dict(),
            'critic_optimizer_state_dict': self.agent.critic_optimizer.state_dict(),
            'config': self.config,
            'training_stats': self.training_stats,
            'total_steps': self.agent.total_steps
        }
        
        if self.config.auto_alpha:
            checkpoint['log_alpha'] = self.agent.log_alpha
            checkpoint['alpha_optimizer_state_dict'] = self.agent.alpha_optimizer.state_dict()
        
        if is_final:
            filename = f"{self.config.output_dir}/sac_final_model.pth"
        else:
            filename = f"{self.config.output_dir}/sac_checkpoint_epoch_{epoch}.pth"
        
        torch.save(checkpoint, filename)
        logger.info(f"ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹: {filename}")
    
    def save_training_report(self):
        """ä¿å­˜è®­ç»ƒæŠ¥å‘Š"""
        report = {
            'training_config': {
                'rl_epochs': self.config.rl_epochs,
                'sac_lr': self.config.sac_lr,
                'tau': self.config.tau,
                'alpha': self.config.alpha,
                'gamma': self.config.gamma,
                'replay_buffer_size': self.config.replay_buffer_size,
                'batch_size': self.config.batch_size
            },
            'training_results': {
                'final_avg_reward': np.mean(self.training_stats['episode_rewards'][-100:]) if self.training_stats['episode_rewards'] else 0,
                'final_success_rate': self.training_stats['success_rates'][-1] if self.training_stats['success_rates'] else 0,
                'final_eval_score': self.training_stats['evaluation_scores'][-1] if self.training_stats['evaluation_scores'] else 0,
                'total_episodes': len(self.training_stats['episode_rewards'])
            },
            'training_stats': self.training_stats
        }
        
        report_file = f"{self.config.output_dir}/sac_training_report.json"
        with open(report_file, 'w') as f:
            json.dump(report, f, indent=2)
        
        logger.info(f"ğŸ“‹ è®­ç»ƒæŠ¥å‘Šå·²ä¿å­˜: {report_file}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨SACå¼ºåŒ–å­¦ä¹ è®­ç»ƒ")
    print("=" * 80)
    
    # åˆ›å»ºé…ç½®
    config = SACConfig()
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = SACTrainer(config)
    
    # å¼€å§‹è®­ç»ƒ
    trainer.train()
    
    print("ğŸ‰ SACå¼ºåŒ–å­¦ä¹ è®­ç»ƒå®Œæˆ!")
    print(f"ğŸ“Š ç»“æœä¿å­˜åœ¨: {config.output_dir}")

if __name__ == "__main__":
    main()
